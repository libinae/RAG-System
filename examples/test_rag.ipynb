{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"  # Show all outputs in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRAG系统测试脚本\\n\\n这个脚本用于测试RAG系统的各个组件和整体功能。\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "RAG系统测试脚本\n",
    "\n",
    "这个脚本用于测试RAG系统的各个组件和整体功能。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 添加项目根目录到系统路径\n",
    "project_root = Path('../')  # Since this variable is already defined with value WindowsPath('.') in another cell\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告: FAISS未安装，将无法使用FAISSVectorStore\n",
      "警告: ChromaDB未安装，将无法使用ChromaVectorStore\n"
     ]
    }
   ],
   "source": [
    "# 导入自定义模块\n",
    "#sys.path.append(\"..\")\n",
    "\n",
    "from src.document_loader import load_documents\n",
    "from src.text_chunker import split_documents, ChunkerFactory\n",
    "from src.embeddings import generate_embeddings, EmbedderFactory\n",
    "from src.vector_store import SimpleVectorStore, FAISSVectorStore, ChromaVectorStore\n",
    "from src.retriever import RetrieverFactory\n",
    "from src.generator import GeneratorFactory\n",
    "from src.rag_pipeline import RAGPipeline, create_rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_document_loading():\n",
    "    \"\"\"测试文档加载功能\"\"\"\n",
    "    print(\"\\n===== 测试文档加载 =====\")\n",
    "    data_dir = os.path.join(project_root, \"data\")\n",
    "    os.listdir(data_dir)\n",
    "    documents = load_documents(data_dir)\n",
    "    \n",
    "    print(f\"加载了 {len(documents)} 个文档\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"文档 {i+1}:\")\n",
    "        print(f\"  - 文件名: {doc['metadata']['source']}\")\n",
    "        print(f\"  - 内容长度: {len(doc['content'])} 字符\")\n",
    "        print(f\"  - 内容预览: {doc['content'][:100]}...\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 测试文档加载 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载文档: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 2 个文档\n",
      "文档 1:\n",
      "  - 文件名: ..\\data\\test_document1.txt\n",
      "  - 内容长度: 629 字符\n",
      "  - 内容预览: 人工智能简介 人工智能Artificial Intelligence简称AI是计算机科学的一个分支它致力于研究和开发能够模拟延伸和扩展人类智能的理论方法技术及应用系统  人工智能的主要领域  机器学习...\n",
      "文档 2:\n",
      "  - 文件名: ..\\data\\test_document2.txt\n",
      "  - 内容长度: 823 字符\n",
      "  - 内容预览: 大语言模型简介 大语言模型Large Language Models简称LLMs是一种基于深度学习的自然语言处理模型它们通过在海量文本数据上训练能够理解和生成人类语言  大语言模型的发展历程 大语言模...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': '人工智能简介 人工智能Artificial Intelligence简称AI是计算机科学的一个分支它致力于研究和开发能够模拟延伸和扩展人类智能的理论方法技术及应用系统  人工智能的主要领域  机器学习 机器学习是人工智能的核心它使计算机能够从数据中学习而无需明确编程主要方法包括监督学习无监督学习和强化学习  深度学习 深度学习是机器学习的一个子集它使用多层神经网络来模拟人脑的工作方式深度学习在图像识别语音识别和自然语言处理等领域取得了突破性进展  自然语言处理 自然语言处理NLP使计算机能够理解解释和生成人类语言它是语音助手机器翻译和情感分析等应用的基础  计算机视觉 计算机视觉使机器能够从图像或视频中获取信息理解视觉世界它广泛应用于人脸识别自动驾驶和医学影像分析等领域  人工智能的应用 1. 医疗保健疾病诊断药物发现个性化治疗 2. 金融风险评估欺诈检测算法交易 3. 交通自动驾驶交通流量优化路线规划 4. 教育个性化学习自动评分智能辅导 5. 制造业预测性维护质量控制供应链优化  人工智能的挑战与伦理问题 尽管人工智能带来了巨大的机遇但也面临着诸多挑战和伦理问题如隐私保护算法偏见就业影响和安全风险等解决这些问题需要技术专家政策制定者和社会各界的共同努力  人工智能的未来 随着技术的不断进步人工智能将继续改变我们的生活和工作方式通用人工智能AGI和超级人工智能ASI的发展可能会带来更加深远的影响人类需要谨慎而明智地引导这一技术的发展方向',\n",
       " 'metadata': {'source': '..\\\\data\\\\test_document1.txt',\n",
       "  'filename': 'test_document1.txt',\n",
       "  'filetype': 'txt'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = test_document_loading()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_chunking(documents):\n",
    "    \"\"\"测试文本分块功能\"\"\"\n",
    "    print(\"\\n===== 测试文本分块 =====\")\n",
    "    \n",
    "    # 测试不同的分块方法\n",
    "    chunker_types = [\"paragraph\", \"fixed_size\"]\n",
    "    chunk_sizes = [500, 1000]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for chunker_type in chunker_types:\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n使用 {chunker_type} 分块器，块大小 {chunk_size}:\")\n",
    "            \n",
    "            # 分块\n",
    "            start_time = time.time()\n",
    "            chunks = split_documents(\n",
    "                documents,\n",
    "                chunker_type=chunker_type,\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=100\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            print(f\"  - 生成了 {len(chunks)} 个文本块\")\n",
    "            print(f\"  - 处理时间: {end_time - start_time:.4f} 秒\")\n",
    "            \n",
    "            # 显示一些块的信息\n",
    "            if chunks:\n",
    "                print(f\"  - 第一个块大小: {len(chunks[0]['content'])} 字符\")\n",
    "                print(f\"  - 第一个块内容预览: {chunks[0]['content'][:100]}...\")\n",
    "            \n",
    "            results[(chunker_type, chunk_size)] = chunks\n",
    "    \n",
    "    # 返回段落分块器、块大小1000的结果用于后续测试\n",
    "    return results.get((\"paragraph\", 1000), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embeddings(chunks):\n",
    "    \"\"\"测试向量嵌入功能\"\"\"\n",
    "    print(\"\\n===== 测试向量嵌入 =====\")\n",
    "    \n",
    "    # 使用sentence_transformer嵌入器\n",
    "    embedder_type = \"sentence_transformer\"\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    \n",
    "    print(f\"使用 {embedder_type} 嵌入器，模型 {model_name}:\")\n",
    "    \n",
    "    # 创建嵌入器\n",
    "    embedder = EmbedderFactory.get_embedder(\n",
    "        embedder_type=embedder_type,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # 生成嵌入向量\n",
    "    start_time = time.time()\n",
    "    chunks_with_embeddings = embedder.generate_embeddings(chunks)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"  - 为 {len(chunks_with_embeddings)} 个文本块生成了嵌入向量\")\n",
    "    print(f\"  - 处理时间: {end_time - start_time:.4f} 秒\")\n",
    "    \n",
    "    # 显示嵌入向量的维度\n",
    "    if chunks_with_embeddings:\n",
    "        embedding_dim = len(chunks_with_embeddings[0]['embedding'])\n",
    "        print(f\"  - 嵌入向量维度: {embedding_dim}\")\n",
    "    \n",
    "    return chunks_with_embeddings\n",
    "\n",
    "\n",
    "def test_vector_store(chunks_with_embeddings):\n",
    "    \"\"\"测试向量存储功能\"\"\"\n",
    "    print(\"\\n===== 测试向量存储 =====\")\n",
    "    \n",
    "    # 测试SimpleVectorStore\n",
    "    print(\"\\n测试 SimpleVectorStore:\")\n",
    "    simple_store = SimpleVectorStore()\n",
    "    \n",
    "    # 添加文档\n",
    "    start_time = time.time()\n",
    "    simple_store.add_documents(chunks_with_embeddings)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"  - 添加了 {len(chunks_with_embeddings)} 个文档到向量存储\")\n",
    "    print(f\"  - 处理时间: {end_time - start_time:.4f} 秒\")\n",
    "    \n",
    "    # 测试搜索\n",
    "    if chunks_with_embeddings:\n",
    "        query_vector = chunks_with_embeddings[0]['embedding']  # 使用第一个文档的向量作为查询\n",
    "        results = simple_store.search(query_vector, top_k=3)\n",
    "        \n",
    "        print(f\"  - 搜索结果数量: {len(results)}\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"    结果 {i+1}: 相似度分数 = {result['score']:.4f}\")\n",
    "            print(f\"    内容预览: {result['content'][:100]}...\")\n",
    "    \n",
    "    # 测试保存和加载\n",
    "    save_dir = os.path.join(project_root, \"examples\", \"test_vector_store\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存\n",
    "    simple_store.save(save_dir)\n",
    "    print(f\"  - 向量存储已保存到 {save_dir}\")\n",
    "    \n",
    "    # 加载\n",
    "    new_store = SimpleVectorStore()\n",
    "    new_store.load(save_dir)\n",
    "    print(f\"  - 从 {save_dir} 加载了向量存储\")\n",
    "    print(f\"  - 加载的文档数量: {len(new_store.documents)}\")\n",
    "    \n",
    "    # 如果有FAISS可用，也测试FAISSVectorStore\n",
    "    try:\n",
    "        import faiss\n",
    "        print(\"\\n测试 FAISSVectorStore:\")\n",
    "        faiss_store = FAISSVectorStore()\n",
    "        \n",
    "        # 添加文档\n",
    "        start_time = time.time()\n",
    "        faiss_store.add_documents(chunks_with_embeddings)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"  - 添加了 {len(chunks_with_embeddings)} 个文档到FAISS向量存储\")\n",
    "        print(f\"  - 处理时间: {end_time - start_time:.4f} 秒\")\n",
    "        \n",
    "        # 测试搜索\n",
    "        if chunks_with_embeddings:\n",
    "            query_vector = chunks_with_embeddings[0]['embedding']\n",
    "            results = faiss_store.search(query_vector, top_k=3)\n",
    "            \n",
    "            print(f\"  - 搜索结果数量: {len(results)}\")\n",
    "            for i, result in enumerate(results):\n",
    "                print(f\"    结果 {i+1}: 相似度分数 = {result['score']:.4f}\")\n",
    "    except ImportError:\n",
    "        print(\"FAISS未安装，跳过FAISSVectorStore测试\")\n",
    "    \n",
    "    return simple_store\n",
    "\n",
    "\n",
    "def test_retriever(vector_store, chunks_with_embeddings):\n",
    "    \"\"\"测试检索器功能\"\"\"\n",
    "    print(\"\\n===== 测试检索器 =====\")\n",
    "    \n",
    "    # 创建检索器\n",
    "    retriever = RetrieverFactory.get_retriever(\n",
    "        retriever_type=\"vector\",\n",
    "        vector_store=vector_store,\n",
    "        embedder_type=\"sentence_transformer\",\n",
    "        model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    )\n",
    "    \n",
    "    # 测试检索\n",
    "    test_queries = [\n",
    "        \"什么是人工智能？\",\n",
    "        \"大语言模型有哪些局限性？\",\n",
    "        \"RAG系统如何工作？\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n查询: '{query}'\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = retriever.retrieve(query, top_k=3)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"  - 检索时间: {end_time - start_time:.4f} 秒\")\n",
    "        print(f\"  - 检索结果数量: {len(results)}\")\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"    结果 {i+1}: 相似度分数 = {result['score']:.4f}\")\n",
    "            print(f\"    内容预览: {result['content'][:100]}...\")\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "def test_generator(retriever):\n",
    "    \"\"\"测试生成器功能\"\"\"\n",
    "    print(\"\\n===== 测试生成器 =====\")\n",
    "    \n",
    "    # 创建模板生成器\n",
    "    generator = GeneratorFactory.get_generator(generator_type=\"template\")\n",
    "    \n",
    "    # 测试生成\n",
    "    test_queries = [\n",
    "        \"什么是人工智能？\",\n",
    "        \"大语言模型有哪些局限性？\",\n",
    "        \"RAG系统如何工作？\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n查询: '{query}'\")\n",
    "        \n",
    "        # 检索相关文档\n",
    "        retrieved_docs = retriever.retrieve(query, top_k=3)\n",
    "        \n",
    "        # 生成回答\n",
    "        start_time = time.time()\n",
    "        answer = generator.generate(query, retrieved_docs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"  - 生成时间: {end_time - start_time:.4f} 秒\")\n",
    "        print(f\"  - 生成的回答:\\n{answer}\")\n",
    "\n",
    "\n",
    "def test_rag_pipeline():\n",
    "    \"\"\"测试完整的RAG流程\"\"\"\n",
    "    print(\"\\n===== 测试完整RAG流程 =====\")\n",
    "    \n",
    "    # 创建RAG流程\n",
    "    config_path = os.path.join(project_root, \"config.json\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    rag = RAGPipeline(config)\n",
    "    \n",
    "    # 索引文档\n",
    "    data_dir = os.path.join(project_root, \"data\")\n",
    "    print(f\"\\n索引文档目录: {data_dir}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    rag.index_documents(data_dir)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"  - 索引时间: {end_time - start_time:.4f} 秒\")\n",
    "    \n",
    "    # 测试查询\n",
    "    test_queries = [\n",
    "        \"什么是人工智能？\",\n",
    "        \"大语言模型有哪些局限性？\",\n",
    "        \"RAG系统如何工作？\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n查询: '{query}'\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        answer = rag.query(query)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"  - 查询时间: {end_time - start_time:.4f} 秒\")\n",
    "        print(f\"  - 生成的回答:\\n{answer}\")\n",
    "    \n",
    "    # 测试保存和加载\n",
    "    save_dir = os.path.join(project_root, \"examples\", \"test_rag_pipeline\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存\n",
    "    rag.save(save_dir)\n",
    "    print(f\"\\nRAG流程已保存到 {save_dir}\")\n",
    "    \n",
    "    # 加载\n",
    "    new_rag = create_rag_pipeline()\n",
    "    new_rag.load(save_dir)\n",
    "    print(f\"从 {save_dir} 加载了RAG流程\")\n",
    "    \n",
    "    # 测试加载后的查询\n",
    "    query = \"RAG系统的优势是什么？\"\n",
    "    print(f\"\\n加载后查询: '{query}'\")\n",
    "    \n",
    "    answer = new_rag.query(query)\n",
    "    print(f\"  - 生成的回答:\\n{answer}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试RAG系统...\n",
      "\n",
      "\n",
      "===== 测试文档加载 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载文档: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 2 个文档\n",
      "文档 1:\n",
      "  - 文件名: ..\\data\\test_document1.txt\n",
      "  - 内容长度: 629 字符\n",
      "  - 内容预览: 人工智能简介 人工智能Artificial Intelligence简称AI是计算机科学的一个分支它致力于研究和开发能够模拟延伸和扩展人类智能的理论方法技术及应用系统  人工智能的主要领域  机器学习...\n",
      "文档 2:\n",
      "  - 文件名: ..\\data\\test_document2.txt\n",
      "  - 内容长度: 823 字符\n",
      "  - 内容预览: 大语言模型简介 大语言模型Large Language Models简称LLMs是一种基于深度学习的自然语言处理模型它们通过在海量文本数据上训练能够理解和生成人类语言  大语言模型的发展历程 大语言模...\n",
      "\n",
      "===== 测试文本分块 =====\n",
      "\n",
      "使用 paragraph 分块器，块大小 500:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    print(\"开始测试RAG系统...\\n\")\n",
    "    \n",
    "    # 测试各个组件\n",
    "    documents = test_document_loading()\n",
    "    chunks = test_text_chunking(documents[0])\n",
    "    chunks_with_embeddings = test_embeddings(chunks)\n",
    "    vector_store = test_vector_store(chunks_with_embeddings)\n",
    "    retriever = test_retriever(vector_store, chunks_with_embeddings)\n",
    "    test_generator(retriever)\n",
    "    \n",
    "    # 测试完整流程\n",
    "    test_rag_pipeline()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
